{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fbc5d13",
   "metadata": {},
   "source": [
    "# Clasificador de Instrumentos Musicales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc1113e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencias\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Asegura el uso de la GPU si está disponible\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b38f26",
   "metadata": {},
   "source": [
    "## Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e5b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "SAMPLE_RATE = 22050\n",
    "MAX_LEN = 3  # segundos\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 3e-4\n",
    "DATA_DIR = r\"C:\\Users\\saray\\Downloads\\oyopf\\all-sample-des\"  # Cambiar según ruta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5e280",
   "metadata": {},
   "source": [
    "## Procesamiento de Audio\n",
    "Clase que transforma archivos de audio a espectrogramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add71ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Procesamiento de Audio\n",
    "class AudioProcessor:\n",
    "    \"\"\"\n",
    "    Clase encargada del procesamiento de audio crudo (forma de onda)\n",
    "    convirtiéndolo en un espectrograma de Mel normalizado en decibelios.\n",
    "\n",
    "    Este procesamiento es comúnmente utilizado como entrada para redes\n",
    "    neuronales en tareas de clasificación o detección de eventos de audio.\n",
    "\n",
    "    Atributos:\n",
    "        sample_rate (int): Frecuencia de muestreo esperada de los audios.\n",
    "        n_mels (int): Número de bandas de Mel a generar.\n",
    "        mel_transform (MelSpectrogram): Transformador de onda a Mel.\n",
    "        db_transform (AmplitudeToDB): Convertidor de amplitud a decibelios.\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_rate=22050, n_mels=128):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        #Transformar ondas a un espectograma\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=2048, #Fourier para análisis\n",
    "            hop_length=512, #Pasos\n",
    "            n_mels=n_mels,\n",
    "            power=2\n",
    "        )\n",
    "        #Amplitudes a escala logarítmica de decibelios\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "    def process(self, waveform, sample_rate):\n",
    "        \"\"\"\n",
    "        Procesa una forma de onda: la re-muestrea, normaliza y transforma\n",
    "        en un espectrograma de Mel logarítmico y normalizado.\n",
    "\n",
    "        Args:\n",
    "            waveform (Tensor): Tensor de forma [canales, muestras].\n",
    "            sample_rate (int): Frecuencia de muestreo original del audio.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Espectrograma de Mel de forma [1, n_mels, tiempo].\n",
    "        \"\"\"\n",
    "        #Poner todo en una frecuencia\n",
    "        if sample_rate != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        #Cambiar a monoaudio\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        #Definir una duración    \n",
    "        target_samples = int(self.sample_rate * MAX_LEN)\n",
    "        if waveform.shape[1] > target_samples:\n",
    "            waveform = waveform[:, :target_samples]\n",
    "        else:\n",
    "            pad_amount = target_samples - waveform.shape[1]\n",
    "            waveform = F.pad(waveform, (0, pad_amount))\n",
    "        \n",
    "        waveform = waveform / waveform.abs().max()\n",
    "        mel = self.mel_transform(waveform)\n",
    "        mel_db = self.db_transform(mel)\n",
    "        mel_db = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-8)\n",
    "        return mel_db\n",
    "\n",
    "# 2. Dataset\n",
    "class InstrumentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personalizado para audios de instrumentos musicales,\n",
    "    organizados por carpetas (una por clase).\n",
    "\n",
    "    Carga archivos .mp3 o .wav, los transforma a espectrogramas de Mel,\n",
    "    y opcionalmente aplica aumento de datos.\n",
    "\n",
    "    Atributos:\n",
    "        processor (AudioProcessor): Instancia para procesar los audios.\n",
    "        augment (bool): Si se debe aplicar aumento de datos o no.\n",
    "        samples (list): Lista de tuplas (ruta_audio, etiqueta).\n",
    "        label_map (dict): Mapeo de índice a nombre de clase.\n",
    "        inverse_map (dict): Mapeo de nombre a índice de clase.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, augment=False, max_samples_per_class=200):\n",
    "        \"\"\"\n",
    "        Inicializa el dataset escaneando las carpetas de clases.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Ruta al directorio que contiene subcarpetas\n",
    "                            (cada una representa una clase).\n",
    "            augment (bool): Si se debe aplicar aumento de datos.\n",
    "            max_samples_per_class (int): Número máximo de archivos por clase.\n",
    "        \"\"\"\n",
    "        self.processor = AudioProcessor(SAMPLE_RATE)\n",
    "        self.augment = augment\n",
    "        self.samples = []\n",
    "        self.label_map = {}\n",
    "        self.inverse_map = {}\n",
    "        \n",
    "        #Analizar subcarpetas\n",
    "        class_dirs = sorted(glob(os.path.join(data_dir, \"*\")))\n",
    "        for class_idx, class_dir in enumerate(class_dirs):\n",
    "            class_name = os.path.basename(class_dir)\n",
    "            self.label_map[class_idx] = class_name\n",
    "            self.inverse_map[class_name] = class_idx\n",
    "            \n",
    "            #Para que lea mp3 o wav\n",
    "            audio_files = glob(os.path.join(class_dir, \"*.mp3\")) + glob(os.path.join(class_dir, \"*.wav\"))\n",
    "            audio_files = audio_files[:max_samples_per_class]\n",
    "            \n",
    "            for audio_file in tqdm(audio_files, desc=f\"Cargando {class_name}\"):\n",
    "                self.samples.append((audio_file, class_idx))\n",
    "        \n",
    "        print(f\"\\nDataset cargado: {len(self.samples)} muestras, {len(self.label_map)} clases\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Devuelve una muestra procesada lista para ser alimentada a un modelo.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Índice de la muestra.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, int]: (espectrograma, etiqueta)\n",
    "        \"\"\"\n",
    "        audio_path, label = self.samples[idx]\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "            spec = self.processor.process(waveform, sample_rate)\n",
    "            #Aumentar datos para entrenar\n",
    "            if self.augment and np.random.random() < 0.5:\n",
    "                spec = torch.roll(spec, shifts=np.random.randint(-10, 10), dims=2)\n",
    "            #Asegurar las dimensiones\n",
    "            if spec.dim() == 2:\n",
    "                spec = spec.unsqueeze(0)\n",
    "            return spec, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {audio_path}: {str(e)}\")\n",
    "            #Si hay errores, el espectograma es blanco\n",
    "            dummy = torch.zeros((1, self.processor.n_mels, int(SAMPLE_RATE * MAX_LEN / 512) + 1))\n",
    "            return dummy, 0\n",
    "\n",
    "    def get_class_weights(self):\n",
    "        \"\"\"\n",
    "        Calcula pesos por clase inversamente proporcionales a su frecuencia.\n",
    "        Esto permite balancear clases desequilibradas durante el entrenamiento.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Pesos normalizados por clase (float32).\n",
    "        \"\"\"\n",
    "        counts = np.bincount([label for _, label in self.samples])\n",
    "        counts = np.where(counts == 0, 1, counts)\n",
    "        weights = 1. / counts\n",
    "        weights = weights / weights.sum() * len(weights)\n",
    "        return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "# 3. Modelo CNN\n",
    "class InstrumentCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo simple de red neuronal convolucional (CNN) para clasificación\n",
    "    de espectrogramas de Mel.\n",
    "\n",
    "    Arquitectura:\n",
    "        - 3 bloques Conv2D + BatchNorm + ReLU + MaxPool\n",
    "        - Flatten + Linear\n",
    "\n",
    "    Atributos:\n",
    "        cnn_layers (Sequential): Capas convolucionales\n",
    "        fc (Linear): Capa final de clasificación\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Tensor de entrada [batch_size, 1, n_mels, tiempo].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Logits por clase.\n",
    "        \"\"\"\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044a81b",
   "metadata": {},
   "source": [
    "## Visualización del Entrenamiento\n",
    "Matriz y gráficas de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94a7224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualización\n",
    "class TrainingVisualizer:\n",
    "    \"\"\"\n",
    "    Clase para visualizar el entrenamiento del modelo, incluyendo:\n",
    "\n",
    "    - Evolución de la pérdida (loss) y precisión (accuracy).\n",
    "    - Matriz de confusión.\n",
    "    - Reporte de clasificación.\n",
    "\n",
    "    Args:\n",
    "        label_map (dict): Diccionario con el mapeo de índices a nombres de clases.\n",
    "    \"\"\"\n",
    "    def __init__(self, label_map):\n",
    "        self.label_map = label_map\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        \n",
    "    def update(self, epoch, tr_loss, val_loss, tr_acc, val_acc, model, val_loader):\n",
    "        \"\"\"\n",
    "        Actualiza las métricas y genera visualizaciones cada 5 épocas o al final.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Número de época actual.\n",
    "            tr_loss (float): Pérdida del conjunto de entrenamiento.\n",
    "            val_loss (float): Pérdida del conjunto de validación.\n",
    "            tr_acc (float): Precisión en entrenamiento.\n",
    "            val_acc (float): Precisión en validación.\n",
    "            model (nn.Module): Modelo entrenado.\n",
    "            val_loader (DataLoader): Dataloader de validación.\n",
    "        \"\"\"\n",
    "        self.train_loss.append(tr_loss)\n",
    "        self.val_loss.append(val_loss)\n",
    "        self.train_acc.append(tr_acc)\n",
    "        self.val_acc.append(val_acc)\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
    "            self._plot_metrics()\n",
    "            self._plot_confusion_matrix(model, val_loader)\n",
    "    \n",
    "    def _plot_metrics(self):\n",
    "        \"\"\"Genera y guarda un gráfico de la evolución de pérdida y precisión.\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_loss, label='Entrenamiento')\n",
    "        plt.plot(self.val_loss, label='Validación')\n",
    "        plt.title('Evolución de la Pérdida')\n",
    "        plt.xlabel('Época')\n",
    "        plt.ylabel('Pérdida')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.train_acc, label='Entrenamiento')\n",
    "        plt.plot(self.val_acc, label='Validación')\n",
    "        plt.title('Evolución de la Precisión')\n",
    "        plt.xlabel('Época')\n",
    "        plt.ylabel('Precisión')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_metrics.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_confusion_matrix(self, model, loader):\n",
    "        \"\"\"\n",
    "        Genera y guarda la matriz de confusión junto con un reporte de clasificación.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): Modelo entrenado.\n",
    "            loader (DataLoader): Loader del conjunto de validación/test.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=self.label_map.values(),\n",
    "                    yticklabels=self.label_map.values())\n",
    "        plt.title('Matriz de Confusión')\n",
    "        plt.xlabel('Predicciones')\n",
    "        plt.ylabel('Valores Verdaderos')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"\\nReporte de Clasificación:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=self.label_map.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e3386",
   "metadata": {},
   "source": [
    "## Funciones de Entrenamiento y Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dcdaa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Funciones de Entrenamiento\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, label_map):\n",
    "    \"\"\"\n",
    "    Entrena el modelo CNN y guarda los mejores pesos según la precisión en validación.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a entrenar.\n",
    "        train_loader (DataLoader): Datos de entrenamiento.\n",
    "        val_loader (DataLoader): Datos de validación.\n",
    "        criterion (Loss): Función de pérdida.\n",
    "        optimizer (Optimizer): Optimizador.\n",
    "        scheduler (LRScheduler): Planificador de tasa de aprendizaje.\n",
    "        num_epochs (int): Número de épocas.\n",
    "        label_map (dict): Mapeo de clases para visualización.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Modelo entrenado con los mejores pesos.\n",
    "    \"\"\"\n",
    "    visualizer = TrainingVisualizer(label_map)\n",
    "    best_acc = 0.0\n",
    "    epoch_times = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        #Validar\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "        avg_time = sum(epoch_times) / len(epoch_times)\n",
    "        remaining_time = avg_time * (num_epochs - epoch - 1)\n",
    "        \n",
    "        visualizer.update(epoch, train_loss, val_loss, train_acc.item(), val_acc.item(), model, val_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | '\n",
    "              f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | '\n",
    "              f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | '\n",
    "              f'Tiempo: {epoch_time:.1f}s | '\n",
    "              f'ETA: {remaining_time/60:.1f}min')\n",
    "    total_time = sum(epoch_times)\n",
    "    avg_epoch_time = total_time / num_epochs\n",
    "    print(f'\\nEntrenamiento completado en {total_time/60:.1f} minutos')\n",
    "    print(f'Tiempo promedio por época: {avg_epoch_time:.1f} segundos')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_dataloaders(data_dir, batch_size=32, val_split=0.2):\n",
    "    full_dataset = InstrumentDataset(data_dir, augment=True)\n",
    "    \n",
    "    indices = list(range(len(full_dataset)))\n",
    "    labels = [full_dataset.samples[i][1] for i in indices]\n",
    "    \n",
    "    train_indices, val_indices = train_test_split(indices, test_size=val_split, stratify=labels)\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "    \n",
    "    train_labels = [full_dataset.samples[i][1] for i in train_indices]\n",
    "    class_weights = full_dataset.get_class_weights()\n",
    "    sample_weights = class_weights[train_labels]\n",
    "    \n",
    "    if (sample_weights <= 0).any():\n",
    "        sample_weights = torch.clamp(sample_weights, min=1e-8)\n",
    "    \n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader, full_dataset.label_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d6825",
   "metadata": {},
   "source": [
    "## Nuevos Audios\n",
    "No lo he probado bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b8c102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Clasificación de Audios Nuevos\n",
    "def predict_audio(file_path, model_path='best_model.pth', threshold=0.6, show_spectrogram=True):\n",
    "    \"\"\"\n",
    "    Clasifica un archivo de audio y visualiza el espectrograma y la probabilidad por clase.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Ruta al archivo de audio (.wav).\n",
    "        model_path (str): Ruta al modelo entrenado.\n",
    "        threshold (float): Umbral de confianza para la predicción.\n",
    "        show_spectrogram (bool): Si se desea mostrar el espectrograma.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, float]: Clase predicha y confianza asociada.\n",
    "    \"\"\"\n",
    "    if not hasattr(predict_audio, 'label_map'):\n",
    "        _, _, predict_audio.label_map = create_dataloaders(DATA_DIR, BATCH_SIZE)\n",
    "        predict_audio.model = InstrumentCNN(len(predict_audio.label_map)).to(DEVICE)\n",
    "        predict_audio.model.load_state_dict(torch.load(model_path))\n",
    "        predict_audio.model.eval()\n",
    "        predict_audio.processor = AudioProcessor(SAMPLE_RATE)\n",
    "    \n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        spec = predict_audio.processor.process(waveform, sample_rate)\n",
    "        spec = spec.unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = predict_audio.model(spec)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            conf, pred = torch.max(probs, 1)\n",
    "            conf = conf.item()\n",
    "            pred_class = predict_audio.label_map[pred.item()]\n",
    "        \n",
    "        print(f\"\\n Audio analizado: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        if conf >= threshold:\n",
    "            print(f\"🎵 Predicción: {pred_class} (Confianza: {conf:.2%})\")\n",
    "        else:\n",
    "            print(f\" Predicción incierta: {pred_class} (Confianza: {conf:.2%} < {threshold:.0%})\")\n",
    "        \n",
    "        print(\"\\nDistribución de probabilidades:\")\n",
    "        for i, prob in enumerate(probs.squeeze().cpu().numpy()):\n",
    "            print(f\"- {predict_audio.label_map[i]}: {prob:.2%}\")\n",
    "        \n",
    "        if show_spectrogram:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.imshow(spec.squeeze().cpu().numpy(), aspect='auto', origin='lower')\n",
    "            plt.title(f\"Espectrograma | Pred: {pred_class} ({conf:.2%})\")\n",
    "            plt.colorbar()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return pred_class, conf\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el audio: {str(e)}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378c1ab",
   "metadata": {},
   "source": [
    "## Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a1dabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MENÚ ===\n",
      "1. Entrenar modelo\n",
      "2. Clasificar un audio\n",
      "3. Salir\n",
      "Opción no válida.\n"
     ]
    }
   ],
   "source": [
    "# 7. Función Principal y Menú\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta el pipeline completo de entrenamiento.\n",
    "    \"\"\"\n",
    "    print(\"=== CLASIFICADOR DE INSTRUMENTOS MUSICALES ===\")\n",
    "    print(f\"Dispositivo: {DEVICE}\")\n",
    "    \n",
    "    print(\"\\nCargando dataset...\")\n",
    "    train_loader, val_loader, label_map = create_dataloaders(DATA_DIR, BATCH_SIZE)\n",
    "    \n",
    "    model = InstrumentCNN(len(label_map)).to(DEVICE)\n",
    "    print(f\"\\nModelo creado con {len(label_map)} clases\")\n",
    "    print(f\"Total parámetros: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    print(\"\\nIniciando entrenamiento...\")\n",
    "    model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, EPOCHS, label_map)\n",
    "    \n",
    "    print(\"\\nEntrenamiento completado!\")\n",
    "    print(f\"Mejores pesos guardados en: best_model.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== MENÚ ===\")\n",
    "    print(\"1. Entrenar modelo\")\n",
    "    print(\"2. Clasificar un audio\")\n",
    "    print(\"3. Salir\")\n",
    "\n",
    "    choice = input(\"Selecciona una opción: \")\n",
    "    if choice == \"1\":\n",
    "        main()\n",
    "    elif choice == \"2\":\n",
    "        path = input(\"Ruta del archivo de audio: \")\n",
    "        if os.path.exists(path):\n",
    "            predict_audio(path)\n",
    "        else:\n",
    "            print(\" El archivo no existe. Verifica la ruta.\")\n",
    "    elif choice == \"3\":\n",
    "        print(\"¡Hasta luego!\")\n",
    "    else:\n",
    "        print(\"Opción no válida.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
